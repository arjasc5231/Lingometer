{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PEsKQpkEJVG"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# colab에서만 사용하는 코드. import될 때 주석처리 되어있어야 한다.\n",
        "\n",
        "# drive mount. colab에 내 구글 드라이브 연결\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# import_ipynb module 설치\n",
        "!pip install import_ipynb\n",
        "\n",
        "# import를 위한 경로이동\n",
        "%cd /content/drive/MyDrive/team_malmungchi/colab/speaker_verification/code\n",
        "!ls\n",
        "\"\"\""
      ],
      "id": "2PEsKQpkEJVG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9febc15e"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "import pickle\n",
        "\n",
        "#sys.path.append(\"/content/drive/MyDrive/team_malmungchi/colab/speaker_verification/code\")\n",
        "import import_ipynb\n",
        "from constants import *\n",
        "import batcher\n",
        "import network\n",
        "import loss\n",
        "import fitter\n",
        "import utils\n",
        "from test_eer import test_frame, test_utt\n",
        "\n",
        "# eager execution 사용. test_step에서 eer 계산에 numpy를 사용하기 때문\n",
        "tf.config.run_functions_eagerly(True)"
      ],
      "id": "9febc15e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dae175d"
      },
      "outputs": [],
      "source": [
        " def train(model_name, batcher_name, loss_name, data_paths, hyper_params={}, pre_checkpoint_dir=None, saving_tag=None, root='/content/drive/MyDrive/team_malmungchi/colab/speaker_verification'):\n",
        "    \"\"\" speaker verification model train + val + test \n",
        "    \n",
        "    학습 설정을 받아 speaker verification 모델을 학습 후 가장 EER이 낮은 모델을 저장\n",
        "    가장 EER이 낮은 모델으로 frame 단위 테스트, utterance 단위 테스트\n",
        "    체크포인트는 {root}/model/{model_name}-{batcher_name}-{loss_name}-{saving_tag}에 {epoch}-{val_eer}.hdf5형식으로 저장\n",
        "    transfer learning을 하고싶다면 pre_checkpoint_dir에 pretrain된 모델의 체크포인트가 존재하는 checkpoint 폴더를 입력(root제외)\n",
        "\n",
        "    Args:\n",
        "      model_name : model name in network.ipynb\n",
        "      batcher_name : batcher name in batcher.ipynb (now naive_batcher or simMat_batcher)\n",
        "      loss_name : loss nane in loss.ipynb (now cross_entropy or simMat_loss)\n",
        "      data_paths : train_dataset_path or [train_dataset_path, val_dataset_path, test_frame_dataset_path, test_utt_dataset_path]  (no root)\n",
        "      hyper_params : dictionary for hyper parameters. {learning_rate, initial_epoch, max_epoch}\n",
        "      pre_checkpoint_dir : in case transfer learning, set this argument for pre-trained model's checkpoint dir (no root)\n",
        "      saving_tag : additional name tag for checkpoint directory\n",
        "      root : root path that contains directories 'model', 'code', 'data'\n",
        "\n",
        "    Returns:\n",
        "      None\n",
        "    \n",
        "    Raises:\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # argument 처리\n",
        "    if not isinstance(data_paths,list):\n",
        "      train_dataset_path = root+'/'+data_paths\n",
        "      val_dataset_path = root+'/data/dataset/val_cpp_517_25_49_40.pickle'\n",
        "      test_frame_dataset_path = root+'/data/dataset/test_cpp_516_25_49_40.pickle'\n",
        "      test_utt_dataset_path = root+'/data/dataset/testUtt_cpp_356_25.pickle'\n",
        "    elif len(data_paths)==1:\n",
        "      train_dataset_path = root+'/'+data_paths[0]\n",
        "      val_dataset_path = root+'/data/dataset/val_cpp_517_25_49_40.pickle'\n",
        "      test_frame_dataset_path = root+'/data/dataset/test_cpp_516_25_49_40.pickle'\n",
        "      test_utt_dataset_path = root+'/data/dataset/testUtt_cpp_356_25.pickle'\n",
        "    elif len(data_paths)==4:\n",
        "      train_dataset_path = root+'/'+data_paths[0]\n",
        "      val_dataset_path = root+'/'+data_paths[1]\n",
        "      test_frame_dataset_path = root+'/'+data_paths[2]\n",
        "      test_utt_dataset_path = root+'/'+data_paths[3]\n",
        "    else: raise Exception('argument data_paths is not right format')\n",
        "\n",
        "    initial_epoch = hyper_params.get('initial_epoch', 0)\n",
        "    learning_rate = hyper_params.get('learning_rate', 0.001)\n",
        "    early_stopping_patience = hyper_params.get('early_stopping_patience', 100)\n",
        "    reduce_lr_patience = hyper_params.get('reduce_lr_patience', 40)\n",
        "    reduce_lr_min_lr = hyper_params.get('reduce_lr_min_lr', 0.00001)\n",
        "    \n",
        "\n",
        "    # Batcher 생성 (train dataset 로드)\n",
        "    print('==================================================')\n",
        "    print('create batcher...')\n",
        "    Batcher = batcher.get_batcher(batcher_name, train_dataset_path)\n",
        "    print('batcher is created')\n",
        "\n",
        "\n",
        "    # validation dataset 로드\n",
        "    print('==================================================')\n",
        "    print('load validation dataset...')\n",
        "    with open(val_dataset_path,\"rb\") as f: val_X = pickle.load(f)\n",
        "    print('validation dataset is loaded')\n",
        "    print('shape of data :', val_X.shape)\n",
        "    \n",
        "\n",
        "    # 모델 생성\n",
        "    print('==================================================')\n",
        "    print('create model...')\n",
        "    Model = network.get_network(model_name)\n",
        "    print(model_name+' is created')\n",
        "    Model.summary()\n",
        "\n",
        "\n",
        "    # optimizer 객체 생성\n",
        "    optimizer = tf.optimizers.Adam(learning_rate)\n",
        "\n",
        "\n",
        "    # loss 객체 생성\n",
        "    Loss = loss.get_loss(loss_name)\n",
        "\n",
        "\n",
        "    # 체크포인트 경로 생성\n",
        "    print('==================================================')\n",
        "    print('preparing checkpoint...')\n",
        "    checkpoint_dir = root+f'/model/{model_name}-{batcher_name}-{loss_name}'\n",
        "    if saving_tag: checkpoint_dir+='-'+saving_tag\n",
        "    # transfer learning의 경우 pretrain된 체크포인트 로드\n",
        "    if pre_checkpoint_dir:  \n",
        "      checkpoint_dir += '--transferedFrom--'+pre_checkpoint_dir.split('/')[-1]\n",
        "      best_ckpt = utils.load_best_checkpoint(root+'/'+pre_checkpoint_dir)\n",
        "      Model.load_weights(root+'/'+pre_checkpoint_dir+'/'+best_ckpt)\n",
        "      print('load pre-trainded checkpoint that model:'+pre_checkpoint_dir.split('/')[-1]+f', epoch:{initial_epoch}, EER:'+best_ckpt.split('-')[-1][:-5])\n",
        "    if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir)\n",
        "    # 이전에 학습중이었던 경우 해당 체크포인트 로드해 이어서 학습\n",
        "    best_ckpt = utils.load_best_checkpoint(checkpoint_dir)\n",
        "    if best_ckpt:\n",
        "      initial_epoch = int(best_ckpt.split('-')[0])\n",
        "      Model.load_weights(checkpoint_dir+'/'+best_ckpt)\n",
        "      print('load exist checkpoint that model:'+checkpoint_dir.split('/')[-1]+f', epoch:{initial_epoch}, EER:'+best_ckpt.split('-')[-1][:-5])\n",
        "    # 체크포인트 객체 생성\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_dir+'/{epoch:05d}-{val_eer:.4f}.hdf5', monitor='val_eer', mode='min', save_best_only=True)\n",
        "    hyper_params['initial_epoch']=initial_epoch\n",
        "\n",
        "\n",
        "    # early_stopping 객체 생성. 30epoch동안 eer이 0.1%도 감소하지 않는다면 중지\n",
        "    early_stopping = EarlyStopping(monitor='val_eer', min_delta=0.001, patience=early_stopping_patience, mode='min', verbose=1)\n",
        "\n",
        "    \n",
        "    # reduce_lr 객체 생성. 10epoch동안 val_eer이 감소하지 않는다면 lr 절반으로 줄이기\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_eer', factor=0.5, patience=reduce_lr_patience, mode='min', min_lr=reduce_lr_min_lr, verbose=1)\n",
        "\n",
        "    \n",
        "    # train model\n",
        "    print('==================================================')\n",
        "    print('start training')\n",
        "    callbacks = [checkpoint, early_stopping, reduce_lr]\n",
        "    fitter.fit(Model, Batcher, val_X, Loss, hyper_params, optimizer, callbacks)\n",
        "    del Batcher\n",
        "    del val_X\n",
        "    print('training is end')\n",
        "\n",
        "\n",
        "    # load best weights\n",
        "    print('==================================================')\n",
        "    best_ckpt = utils.load_best_checkpoint(checkpoint_dir)\n",
        "    Model.load_weights(checkpoint_dir+'/'+best_ckpt)\n",
        "    best_epoch = int(best_ckpt.split('-')[0])\n",
        "    print(f'load best checkpoint that epoch:{best_epoch}, EER:'+best_ckpt.split('-')[-1][:-5])\n",
        "\n",
        "\n",
        "    # test\n",
        "    test_frame(Model,test_frame_dataset_path)\n",
        "    test_utt(Model,test_utt_dataset_path)"
      ],
      "id": "3dae175d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6e6591d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "\"\"\" example\n",
        "\n",
        "# naive train\n",
        "train('ACRNN', 'naive_batcher', 'cross_entropy', 'data/dataset/train_300_200_128_512.pickle')\n",
        "\n",
        "# simMat train\n",
        "train('CNN', 'simMat_batcher', 'simMat_loss', 'data/dataset/train_300_200_128_512.pickle')\n",
        "\n",
        "# transfer learning\n",
        "train('naive_model', 'naive_batcher', 'cross_entropy', 'data/dataset/train_300_200_128_512.pickle')\n",
        "train('naive_model', 'simMat_batcher', 'simMat_loss', 'data/dataset/train_300_200_128_512.pickle', pre_checkpoint_dir='model/naive_model-naive_batcher-cross_entropy')\n",
        "\"\"\""
      ],
      "id": "e6e6591d"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}