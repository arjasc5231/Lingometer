{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjasc5231/Lingometer/blob/speaker_verification/speaker_verification/development.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2PEsKQpkEJVG",
        "outputId": "e6419030-9cdf-4ffd-bb22-089b69b2942f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "id": "2PEsKQpkEJVG"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdDOXg5EEoCE",
        "outputId": "5171ff9f-7100-4d59-aa22-a6f408ce1825"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting import_ipynb\n",
            "  Downloading import_ipynb-0.1.4-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: nbformat in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (5.4.0)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.7/dist-packages (from import_ipynb) (7.9.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (5.1.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.0.10)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (2.6.1)\n",
            "Collecting jedi>=0.10\n",
            "  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.6 MB 31.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (0.2.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.4.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (57.4.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from IPython->import_ipynb) (4.8.0)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->IPython->import_ipynb) (0.8.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (1.15.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->IPython->import_ipynb) (0.2.5)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.3.3)\n",
            "Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (2.16.1)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from nbformat->import_ipynb) (4.11.1)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (0.18.1)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.1.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (22.1.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema>=2.6->nbformat->import_ipynb) (4.12.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat->import_ipynb) (3.8.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->IPython->import_ipynb) (0.7.0)\n",
            "Installing collected packages: jedi, import-ipynb\n",
            "Successfully installed import-ipynb-0.1.4 jedi-0.18.1\n"
          ]
        }
      ],
      "source": [
        "!pip install import_ipynb"
      ],
      "id": "xdDOXg5EEoCE"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6bCCOwfHgx8",
        "outputId": "06a257e9-4ad4-4450-a11a-3188a0a8396f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1rEYIox5uYM9uP2spDttCYCqtt2GvSbn1/2022 기술 혁신 실험/colab/speaker_verification/code\n",
            "batcher.ipynb\t   eval_metrics.ipynb  network.ipynb\n",
            "constants.ipynb    fitter.ipynb        preprocess.ipynb\n",
            "development.ipynb  loss.ipynb\t       validation.ipynb\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/team_malmungchi/colab/speaker_verification/code\n",
        "!ls"
      ],
      "id": "U6bCCOwfHgx8"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9febc15e",
        "outputId": "a89c2c09-5cfb-4fac-860c-7c8475b270ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "importing Jupyter notebook from constants.ipynb\n",
            "importing Jupyter notebook from batcher.ipynb\n",
            "importing Jupyter notebook from network.ipynb\n",
            "importing Jupyter notebook from loss.ipynb\n",
            "importing Jupyter notebook from fitter.ipynb\n",
            "importing Jupyter notebook from validation.ipynb\n",
            "importing Jupyter notebook from eval_metrics.ipynb\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "#sys.path.append(\"/content/drive/MyDrive/team_malmungchi/colab/speaker_verification/code\")\n",
        "import import_ipynb\n",
        "from constants import CHECKPOINTS_DIR, NUM_FRAME, NUM_FBANK\n",
        "import batcher\n",
        "import network\n",
        "import loss\n",
        "import fitter\n",
        "from validation import validate"
      ],
      "id": "9febc15e"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3dae175d"
      },
      "outputs": [],
      "source": [
        " def train(checkpoint_dir, batcher_name, model_name, loss_name, fit_name):\n",
        "    \n",
        "    # Batcher 로드\n",
        "    Batcher = batcher.get_batcher(batcher_name, None, '../data/simMat_online_batcher')\n",
        "    #Batcher = batcher.get_batcher(batcher_name, None, '../data/simMat_batcher')\n",
        "    #Batcher = batcher.get_batcher(batcher_name, None, '../data/naive_batcher')\n",
        "    num_speaker= Batcher.num_speaker\n",
        "\n",
        "    # optimizer, loss 객체 생성\n",
        "    optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "    Loss = loss.get_loss(loss_name)\n",
        "    \n",
        "    # 모델 생성\n",
        "    Model = network.get_network(model_name, num_speaker)\n",
        "    Model.summary()\n",
        "    #Model.compile(optimizer='adam', loss=Loss, metrics=['accuracy']) # 이것도 metric이 경우마다 달라질 수 있겠다. fitter 안으로 넣자.\n",
        "    Model.compile(optimizer='adam', loss=Loss) # simMat은 accuracy가 없음\n",
        "    \n",
        "    # 에폭 설정\n",
        "    initial_epoch, max_epoch = 0, 10 # validation 테스트를 위해 1로 설정 \n",
        "    \n",
        "    \"\"\"\n",
        "    # 체크포인트 로드. TODO : name별로 다른 체크포인트 폴더 만들기.\n",
        "    ensures_dir(CHECKPOINTS_DIR)\n",
        "    pre_training_checkpoint = load_best_checkpoint(CHECKPOINTS_DIR)\n",
        "    if pre_training_checkpoint:\n",
        "        initial_epoch = int(pre_training_checkpoint.split('/')[-1].split('.')[0].split('_')[-1])\n",
        "        Molde.m.load_weights(pre_training_checkpoint)  # latest one.\n",
        "    \n",
        "    # 체크포인트 객체 생성\n",
        "    checkpoint_name = dsm.m.name + '_checkpoint'\n",
        "    checkpoint_filename = os.path.join(CHECKPOINTS_DIR, checkpoint_name + '_{epoch}.h5')\n",
        "    checkpoint = ModelCheckpoint(monitor='val_accuracy', filepath=checkpoint_filename, save_best_only=True)\n",
        "    \n",
        "    # early_stopping 객체 생성\n",
        "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=20, verbose=1, mode='max')\n",
        "\n",
        "    # reduce_lr 객체 생성\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # train\n",
        "    fitter.fit(fit_name, Model, Batcher, loss, initial_epoch, max_epoch)\n",
        "    \n",
        "    # validation\n",
        "    eer = validate(Model)\n",
        "    print(eer)\n",
        "    # eer함수가 단위가 1이 max인듯. (eer=0.13->eer=13%)\n",
        "    \n",
        "    "
      ],
      "id": "3dae175d"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6e6591d",
        "scrolled": true,
        "outputId": "356237f4-4696-471a-ab5f-afec37768c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "load exist data\n",
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 128, 128, 1)]     0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 126, 126, 32)      320       \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 63, 63, 32)       0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 61, 61, 64)        18496     \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 31, 31, 64)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 29, 29, 128)       73856     \n",
            "                                                                 \n",
            " max_pooling2d_2 (MaxPooling  (None, 15, 15, 128)      0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 28800)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 167)               4809767   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,902,439\n",
            "Trainable params: 4,902,439\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5\n",
            "1000/1000 [==============================] - 36s 17ms/step - loss: 32.5180\n",
            "Epoch 2/5\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 29.2594\n",
            "Epoch 3/5\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 28.1945\n",
            "Epoch 4/5\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 27.8062\n",
            "Epoch 5/5\n",
            "1000/1000 [==============================] - 17s 17ms/step - loss: 27.4131\n",
            "0.11328125000000003\n"
          ]
        }
      ],
      "source": [
        "#train(None, 'naive_batcher', 'naive_model', 'cross_entropy', 'naive_fit')\n",
        "\n",
        "#train(None, 'simMat_batcher', 'naive_model', 'simMat_loss', 'simMat_fit')\n",
        "\n",
        "train(None, 'simMat_online_batcher', 'naive_model', 'simMat_loss', 'simMat_online_fit')\n",
        "# 10000 batch (1000steps_per_epoch*10epoch)만큼 학습시킨 결과 loss 31.6347 -> 26.7269,  eer 0.10078124999999999 효과좋다! offline에 비해 overfitting도 안일어난다!\n",
        "# 50000 batch (1000*50) 만큼 학습시킨 결과, loss 31.1683 -> 24.9756, eer 0.13515625\n",
        "# 100000 batch (1000*100) 만큼 학습시킨 결과, loss 30.9971 -> 24.7394, eer 0.15000000000000002. 흐음... 오버피팅인가. 오버피팅이 발생할 수가 있나? 데이터셋을 키워보면 좀 해결되려나\n",
        "# 1000 batch (1000*1)만큼 학습시킨 결과 eer 0.16328125\n",
        "# 5000 batch (1000*5)만큼 학습시킨 결과 eer 0.11328125000000003\n",
        "# 생각보다 overfitting의 영향이 큰데 실험할 떄 epoch을 어떻게 설정하는게 맞을까\n",
        "\n",
        "\n"
      ],
      "id": "e6e6591d"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Lhi32jd5QWqZ"
      },
      "id": "Lhi32jd5QWqZ",
      "execution_count": 6,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "development.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}