{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4419aa6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "import import_ipynb\n",
    "from constants import NUM_FBANK, HOP_LENGTH, NUM_PER_SPEAKER, NUM_FRAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58220255",
   "metadata": {},
   "outputs": [],
   "source": [
    "class naive_batcher:\n",
    "    def __init__(self, input_dir, output_dir):\n",
    "        self.name = 'naive_batcher'\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        if len(os.listdir(self.output_dir))==0: self.generate()\n",
    "        else: print('load exist data')\n",
    "        self.filename = os.listdir(self.output_dir)[0]\n",
    "        self.num_speaker = int(self.filename.split('_')[-4])\n",
    "    \n",
    "    # validation용으로 train때 쓰지 않은 화자를 분리해야 하는데 어디서 하지? preprocessing? gernerate?\n",
    "    def generate(self, num_frame=NUM_FRAME, num_per_speaker=NUM_PER_SPEAKER):\n",
    "        #if len(os.listdir(self.output_dir))!=0: error()\n",
    "        X = []\n",
    "        Y = []\n",
    "        speaker_id = 0\n",
    "        for speaker in os.listdir(self.input_dir):\n",
    "            X_speaker = []\n",
    "            for utt in os.listdir(self.input_dir+'/'+speaker):\n",
    "                spec = np.load(self.input_dir+'/'+speaker+'/'+utt)\n",
    "                for i in range(0,spec.shape[0]-num_frame+1,num_frame): X_speaker.append(spec[i:i+num_frame,:])\n",
    "                if len(X_speaker)>=num_per_speaker: break\n",
    "            if len(X_speaker)<num_per_speaker: continue\n",
    "            X.extend(X_speaker[:num_per_speaker])\n",
    "            Y.extend([speaker_id]*num_per_speaker)\n",
    "            speaker_id+=1\n",
    "        X = np.array(X)\n",
    "        Y = np.array(Y)\n",
    "        \n",
    "        # shuffle utterances\n",
    "        shuffle_idx = np.arange(X.shape[0])\n",
    "        np.random.shuffle(shuffle_idx)\n",
    "        X = X[shuffle_idx]\n",
    "        Y = Y[shuffle_idx]\n",
    "        \n",
    "        # split train/test\n",
    "        split_idx = int(X.shape[0]*0.8)\n",
    "        X_train, X_test, Y_train, Y_test = X[:split_idx], X[split_idx:], Y[:split_idx], Y[split_idx:]\n",
    "        \n",
    "        # print information\n",
    "        print()\n",
    "        print('generate train/test dataset')\n",
    "        print('shape:', X_train.shape(), X_test.shape(), Y_train.shape(), Y_test.shape())\n",
    "        \n",
    "        # save\n",
    "        XY = (X_train, X_test, Y_train, Y_test)\n",
    "        with open(self.output_dir+'/'+f\"{speaker_id}_{NUM_PER_SPEAKER}_{NUM_FRAME}_{NUM_FBANK}.npy\",'wb') as f:\n",
    "            pickle.dump(XY,f)\n",
    "        \n",
    "    def load_data(self):\n",
    "        with open(self.output_dir+'/'+self.filename,\"rb\") as f:\n",
    "            return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25305ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batcher(batcher_name):\n",
    "    if batcher_name=='naive_batcher': return naive_batcher()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb7d8ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
