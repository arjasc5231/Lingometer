{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9febc15e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae175d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "import import_ipynb\n",
    "from constants import CHECKPOINTS_DIR, NUM_FRAME, NUM_FBANK\n",
    "import batcher\n",
    "import net_factory\n",
    "import loss\n",
    "from utils import load_best_checkpoint, ensures_dir\n",
    "\n",
    "\n",
    "def fit_model_softmax(dsm: DeepSpeakerModel, kx_train, ky_train, kx_test, ky_test,\n",
    "                      batch_size=BATCH_SIZE, max_epochs=1000, initial_epoch=0):\n",
    "    checkpoint_name = dsm.m.name + '_checkpoint'\n",
    "    checkpoint_filename = os.path.join(CHECKPOINTS_SOFTMAX_DIR, checkpoint_name + '_{epoch}.h5')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_accuracy', filepath=checkpoint_filename, save_best_only=True)\n",
    "\n",
    "    # if the accuracy does not increase by 0.1% over 20 epochs, we stop the training.\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=20, verbose=1, mode='max')\n",
    "\n",
    "    # if the accuracy does not increase over 10 epochs, we reduce the learning rate by half.\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
    "\n",
    "    max_len_train = len(kx_train) - len(kx_train) % batch_size\n",
    "    kx_train = kx_train[0:max_len_train]\n",
    "    ky_train = ky_train[0:max_len_train]\n",
    "    max_len_test = len(kx_test) - len(kx_test) % batch_size\n",
    "    kx_test = kx_test[0:max_len_test]\n",
    "    ky_test = ky_test[0:max_len_test]\n",
    "\n",
    "    dsm.m.fit(x=kx_train,\n",
    "              y=ky_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=initial_epoch + max_epochs,\n",
    "              initial_epoch=initial_epoch,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(kx_test, ky_test),\n",
    "              callbacks=[early_stopping, reduce_lr, checkpoint])\n",
    "\n",
    "def train_simMat(working_dir):\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def train(working_dir, batcher_name, model_name, loss_name, fit_name):\n",
    "    \n",
    "    # Batcher 로드\n",
    "    Batcher = batcher(batcher_name)\n",
    "    num_speaker= len(Batcher.num_speaker)\n",
    "    \n",
    "    # 모델 생성\n",
    "    Model = net_factory(model_name)\n",
    "    Model.m.compile(optimizer='adam', loss=loss(loss_name))\n",
    "    \n",
    "    # 에폭 설정\n",
    "    initial_epoch, max_epoch = 0, 1000\n",
    "    \n",
    "    # 체크포인트 로드. TODO : name별로 다른 체크포인트 폴더 만들기.\n",
    "    ensures_dir(CHECKPOINTS_DIR)\n",
    "    pre_training_checkpoint = load_best_checkpoint(CHECKPOINTS_DIR)\n",
    "    if pre_training_checkpoint:\n",
    "        initial_epoch = int(pre_training_checkpoint.split('/')[-1].split('.')[0].split('_')[-1])\n",
    "        Molde.m.load_weights(pre_training_checkpoint)  # latest one.\n",
    "    \n",
    "    # 체크포인트 객체 생성\n",
    "    checkpoint_name = dsm.m.name + '_checkpoint'\n",
    "    checkpoint_filename = os.path.join(CHECKPOINTS_DIR, checkpoint_name + '_{epoch}.h5')\n",
    "    checkpoint = ModelCheckpoint(monitor='val_accuracy', filepath=checkpoint_filename, save_best_only=True)\n",
    "\n",
    "    # early_stopping 객체 생성\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', min_delta=0.001, patience=20, verbose=1, mode='max')\n",
    "\n",
    "    # reduce_lr 객체 생성\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_accuracy', factor=0.5, patience=10, min_lr=0.0001, verbose=1)\n",
    "    \n",
    "    Model.m.fit(x=Batcher.train_generator(), y=None, steps_per_epoch=2000, shuffle=False,\n",
    "              epochs=1000, validation_data=Batcher.test_generator(), validation_steps=len(test_batches),\n",
    "              callbacks=[checkpoint])\n",
    "    \n",
    "    # 실전에서 등록 화자의 발화들의 평균과 평가 발화를 비교하는 것도 필요하겠다. 따로 loss를 정의하고 m.evaluation해야 하려나\n",
    "    \n",
    "    dsm.m.fit(x=kx_train,\n",
    "              y=ky_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=initial_epoch + max_epochs,\n",
    "              initial_epoch=initial_epoch,\n",
    "              verbose=1,\n",
    "              shuffle=True,\n",
    "              validation_data=(kx_test, ky_test),\n",
    "              callbacks=[early_stopping, reduce_lr, checkpoint])\n",
    "    \n",
    "    \n",
    "    max_epoch = 1000\n",
    "    \"\"\"\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
